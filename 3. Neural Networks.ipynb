{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. Neural Networks.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN+Z4Xh3dubRDow5tGAHqgw"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KN6r7yq-iNxW","colab_type":"text"},"source":["# NEURAL NETWORKS\n","\n","Neural networks can be constructed using the `torch.nn` package.\n","\n","Now that you had a glimpse of `autograd`, `nn` depends on `autograd` to define models and differentiate them. An `nn.Module` contains layers, and a method `forward(input)` that returns the `output`.\n","\n","For example, look at this network that classifies digit images:\n","\n","![alt text](https://drive.google.com/uc?id=1PZFwMa7KQysweAsw4f9xte7QsvZhX8fj)\n","\n","This is an example of LeNet - a popular CNN architecture.\n","\n","It is a simple feed-forward network. It takes the input, feeds it through several layers one after the other, and then finally gives the output.\n","\n","A typical training procedure for a neural network is as follows:\n","\n","\n","\n","* Define the neural network that has some learnable parameters (or weights)\n","* Iterate over a dataset of inputs\n","* Process input through the network\n","* Compute the loss (how far is the output from being correct)\n","* Propagate gradients back into the network’s parameters \n","* Update the weights of the network, typically using a simple update rule: `weight = weight - learning_rate * gradient`   \n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iP0up9kBjpeg","colab_type":"text"},"source":["## Define the network\n","\n","Let’s define this network:"]},{"cell_type":"code","metadata":{"id":"6twraGywjtZ8","colab_type":"code","outputId":"1c7fd604-3231-4163-da5c-0e1377b3c88d","executionInfo":{"status":"ok","timestamp":1590317522266,"user_tz":-330,"elapsed":3639,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Net(nn.Module):\n","\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # 1 input image channel, 6 output channels, 3x3 square convolution\n","        # kernel\n","        self.conv1 = nn.Conv2d(1, 6, 3)\n","        self.conv2 = nn.Conv2d(6, 16, 3)\n","        # an affine operation: y = Wx + b\n","        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        # Max pooling over a (2, 2) window\n","        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n","        # If the size is a square you can only specify a single number\n","        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n","        x = x.view(-1, self.num_flat_features(x)) \n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","    def num_flat_features(self, x):\n","        size = x.size()[1:]  # all dimensions except the batch dimension\n","        num_features = 1\n","        for s in size:\n","            num_features *= s\n","        return num_features\n","\n","\n","net = Net()\n","print(net)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Net(\n","  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n","  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n","  (fc1): Linear(in_features=576, out_features=120, bias=True)\n","  (fc2): Linear(in_features=120, out_features=84, bias=True)\n","  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QSKPqhUlrDOT","colab_type":"text"},"source":["The output volume after each layer of the above network can be calculated as follows :\n","\n","![output_volume](https://drive.google.com/uc?id=178VXr-SHBw7wbrs0Ev1TWpmIs3CXlrid)"]},{"cell_type":"markdown","metadata":{"id":"d5j098K4kKBU","colab_type":"text"},"source":["You just have to define the `forward` function, and the `backward` function (where gradients are computed) is automatically defined for you using `autograd`. You can use any of the Tensor operations in the `forward` function."]},{"cell_type":"markdown","metadata":{"id":"U1GN1ElGbnms","colab_type":"text"},"source":["Read the following threads to understand the difference between `torch.nn` and `torch.nn.functional`.\n","\n","\n","* https://discuss.pytorch.org/t/beginner-should-relu-sigmoid-be-called-in-the-init-method/18689/5\n","* https://discuss.pytorch.org/t/how-to-choose-between-torch-nn-functional-and-torch-nn-module/2800\n","* https://discuss.pytorch.org/t/difference-of-methods-between-torch-nn-and-functional/1076\n","* https://discuss.pytorch.org/t/what-is-the-difference-between-torch-nn-and-torch-nn-functional/33597\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mr0b1uOccdhk","colab_type":"text"},"source":["The learnable parameters of a model are returned by `net.parameters()`."]},{"cell_type":"code","metadata":{"id":"8LXvVwpWccm1","colab_type":"code","outputId":"e62688f4-4bcd-4721-9cff-521b5107e12f","executionInfo":{"status":"ok","timestamp":1590317522269,"user_tz":-330,"elapsed":3625,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["params = list(net.parameters())\n","# My understanding as to why the total learnable parameters are 10 is : \n","# There are total 5 layers : conv1, conv2, fc1, fc2 and fc3\n","# Each layer has both weights and biases.\n","# Hence, the set of learnable params has a cardinality equal to 10.\n","print(len(params))\n","print(params[2].size())  # conv2's .weight\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["10\n","torch.Size([16, 6, 3, 3])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YbeqO-3Icj92","colab_type":"text"},"source":["Let’s try a random 32x32 input. Note: expected input size of this net (LeNet) is 32x32. To use this net on the MNIST dataset, please resize the images from the dataset to 32x32."]},{"cell_type":"code","metadata":{"id":"AUah0z3WcnWk","colab_type":"code","outputId":"c3af1874-958d-460f-c99f-a21411a6d77a","executionInfo":{"status":"ok","timestamp":1590317522271,"user_tz":-330,"elapsed":3605,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["input = torch.randn(1, 1, 32, 32)\n","out = net(input)\n","print(out)\n","print(out.size())"],"execution_count":3,"outputs":[{"output_type":"stream","text":["tensor([[ 0.0633,  0.0503,  0.1987,  0.1032, -0.0831, -0.0375, -0.0381, -0.0798,\n","         -0.0094,  0.0943]], grad_fn=<AddmmBackward>)\n","torch.Size([1, 10])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"beseNC4Bcurd","colab_type":"text"},"source":["Zero the gradient buffers of all parameters and backprops with random gradients:"]},{"cell_type":"code","metadata":{"id":"roKGMm52cw8k","colab_type":"code","colab":{}},"source":["net.zero_grad()\n","out.backward(torch.randn(1, 10))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"liuQKsozczPN","colab_type":"text"},"source":["**Note:** \n","\n","`torch.nn` only supports mini-batches. The entire `torch.nn` package only supports inputs that are a mini-batch of samples, and not a single sample.\n","\n","For example, `nn.Conv2d` will take in a 4D Tensor of `nSamples x nChannels x Height x Width`.\n","\n","If you have a single sample, just use `input.unsqueeze(0)` to add a fake batch dimension."]},{"cell_type":"code","metadata":{"id":"clLAamjJM68k","colab_type":"code","outputId":"43c1bf62-beda-4ecd-a669-f002b39e0cc8","executionInfo":{"status":"ok","timestamp":1590317522274,"user_tz":-330,"elapsed":3584,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["inp = torch.randn(3, 32, 32)\n","print(inp.size())\n","print(inp.unsqueeze(0).size())"],"execution_count":5,"outputs":[{"output_type":"stream","text":["torch.Size([3, 32, 32])\n","torch.Size([1, 3, 32, 32])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rUs498ksdGWs","colab_type":"text"},"source":["Before proceeding further, let’s recap all the classes you’ve seen so far.\n","\n","**Recap:**\n","\n","* `torch.Tensor` - A multi-dimensional array with support for autograd operations like `backward()`. Also holds the gradient w.r.t. the tensor.\n","\n","* `nn.Module` - Neural network module. Convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.\n","\n","* `nn.Parameter` - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a `Module`.\n","\n","* `autograd.Function` - Implements forward and backward definitions of an autograd operation. Every `Tensor` operation creates at least a single `Function` node that connects to functions that created a `Tensor` and encodes its history.\n","\n","At this point, we covered:\n","\n","* Defining a neural network\n","* Processing inputs and calling backward\n","\n","Still Left:\n","\n","* Computing the loss\n","* Updating the weights of the network"]},{"cell_type":"markdown","metadata":{"id":"oeTW97npNOYv","colab_type":"text"},"source":["## Loss Function\n","\n","A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.\n","\n","There are several different [loss functions](https://pytorch.org/docs/nn.html#loss-functions) under the `nn` package . A simple loss is: `nn.MSELoss` which computes the mean-squared error between the input and the target.\n","\n","For example:"]},{"cell_type":"code","metadata":{"id":"NNR7eq3ONjRT","colab_type":"code","outputId":"e6feec16-f5c6-46bc-cbe8-1e945106de82","executionInfo":{"status":"ok","timestamp":1590317522275,"user_tz":-330,"elapsed":3565,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["output = net(input)\n","print(output.size())\n","target = torch.randn(10)  # a dummy target, for example\n","target = target.unsqueeze(0) # make it the same shape as output\n","print(target.size())\n","criterion = nn.MSELoss()\n","\n","loss = criterion(output, target)\n","print(loss)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["torch.Size([1, 10])\n","torch.Size([1, 10])\n","tensor(1.6183, grad_fn=<MseLossBackward>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XlOqIvSuNyzg","colab_type":"text"},"source":["Now, if you follow `loss` in the backward direction, using its `.grad_fn` attribute, you will see a graph of computations that looks like this:\n","\n","![alt text](https://drive.google.com/uc?id=1hRZrEEWmUuPuFqSxN3kONyYReacqB8nA)\n","\n","So, when we call `loss.backward()`, the whole graph is differentiated w.r.t. the loss, and all Tensors in the graph that has `requires_grad=True` will have their `.grad` Tensor accumulated with the gradient.\n","\n","For illustration, let us follow a few steps backward:"]},{"cell_type":"code","metadata":{"id":"Kp43SqVnOfva","colab_type":"code","outputId":"4d752890-ea19-4f18-b332-239c79d3bdab","executionInfo":{"status":"ok","timestamp":1590317522277,"user_tz":-330,"elapsed":3548,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["print(loss.grad_fn)  # MSELoss\n","print(loss.grad_fn.next_functions[0][0])  # Linear\n","print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"],"execution_count":7,"outputs":[{"output_type":"stream","text":["<MseLossBackward object at 0x7f78af45bc18>\n","<AddmmBackward object at 0x7f78af45bc88>\n","<AccumulateGrad object at 0x7f78af45bc18>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qn818bJnOm7Y","colab_type":"text"},"source":["## Backprop\n","\n","To backpropagate the error all we have to do is to `loss.backward()`. You need to clear the existing gradients though, else gradients will be accumulated to existing gradients.\n","\n","Now we shall call `loss.backward()`, and have a look at conv1’s bias gradients before and after the backward."]},{"cell_type":"code","metadata":{"id":"MSxC3Ig5OyMa","colab_type":"code","outputId":"70769700-28d9-4337-b03e-194c6eb96da1","executionInfo":{"status":"ok","timestamp":1590317522278,"user_tz":-330,"elapsed":3531,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["net.zero_grad()     # zeroes the gradient buffers of all parameters\n","\n","print('conv1.bias.grad before backward')\n","print(net.conv1.bias.grad)\n","\n","loss.backward()\n","\n","print('conv1.bias.grad after backward')\n","print(net.conv1.bias.grad)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["conv1.bias.grad before backward\n","tensor([0., 0., 0., 0., 0., 0.])\n","conv1.bias.grad after backward\n","tensor([-0.0243,  0.0183, -0.0081, -0.0105, -0.0183,  0.0118])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V00XzMNPO19A","colab_type":"text"},"source":["Now, we have seen how to use loss functions."]},{"cell_type":"markdown","metadata":{"id":"N_9J5dX7O2nA","colab_type":"text"},"source":["**Read Later:**\n","\n","The neural network package contains various modules and loss functions that form the building blocks of deep neural networks. A full list with documentation is [here](https://pytorch.org/docs/nn).\n","\n","The only thing left to learn is:\n","\n","Updating the weights of the network"]},{"cell_type":"markdown","metadata":{"id":"B5jSFr07PE5A","colab_type":"text"},"source":["## Update the weights\n","\n","The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):\n","\n","`weight = weight - learning_rate * gradient`\n","\n","We can implement this using simple Python code:"]},{"cell_type":"code","metadata":{"id":"LKYYXujSPK5I","colab_type":"code","colab":{}},"source":["learning_rate = 0.01\n","for f in net.parameters():\n","    f.data.sub_(f.grad.data * learning_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r1YSY5A5PyGo","colab_type":"text"},"source":["However, as you use neural networks, you want to use various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. To enable this, we built a small package: `torch.optim` that implements all these methods. Using it is very simple:"]},{"cell_type":"code","metadata":{"id":"rWnKxzmhPwrp","colab_type":"code","colab":{}},"source":["import torch.optim as optim\n","\n","# create your optimizer\n","optimizer = optim.SGD(net.parameters(), lr=0.01)\n","\n","# in your training loop:\n","optimizer.zero_grad()   # zero the gradient buffers\n","output = net(input)\n","loss = criterion(output, target)\n","loss.backward()\n","optimizer.step()    # Does the update"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7lJjghksQBBZ","colab_type":"text"},"source":["**Note:**\n","\n","Observe how gradient buffers had to be manually set to zero using `optimizer.zero_grad()`. This is because gradients are accumulated as explained in the Backprop section."]},{"cell_type":"markdown","metadata":{"id":"VwsbRHbEmMQC","colab_type":"text"},"source":["# References\n","\n","1. [Deep Learning with PyTorch: A 60 minute blitz, Soumith Chintala](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n","\n","2. [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)\n","\n","3. [`TORCH.NN`](https://pytorch.org/docs/stable/nn.html)"]}]}