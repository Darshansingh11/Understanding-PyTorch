{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. Autograd.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNKBjiRutSI6uDZ0fF6iJ7n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"T2TKdEOBG0BV","colab_type":"text"},"source":["# AUTOGRAD: Automatic Differentiation"]},{"cell_type":"markdown","metadata":{"id":"GNpmpYnZHIic","colab_type":"text"},"source":["The PyTorch's `autograd` package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different."]},{"cell_type":"markdown","metadata":{"id":"VDJRsZs6JSeJ","colab_type":"text"},"source":["## Tensors\n","\n","`torch.Tensor` is the central class of the package. If you set its attribute `.requires_grad` as `True`, it starts to track all operations on it. When you finish your computation you can call `.backward()` and have all the gradients computed automatically. The gradient for this tensor will be accumulated into `.grad` attribute.\n","\n","To stop a tensor from tracking history, you can call `.detach()` to detach it from the computation history, and to prevent future computation from being tracked.\n","\n","To prevent tracking history (and using memory), you can also wrap the code block in `with torch.no_grad():`. This can be particularly helpful when evaluating a model because the model may have trainable parameters with `requires_grad=True`, but for which we don’t need the gradients.\n","\n","There’s one more class which is very important for autograd implementation - a `Function`.\n","\n","`Tensor` and `Function` are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each tensor has a `.grad_fn` attribute that references a `Function` that has created the `Tensor` (except for Tensors created by the user - their `grad_fn is None`).\n","\n","If you want to compute the derivatives, you can call `.backward()` on a `Tensor`. If `Tensor` is a scalar (i.e. it holds one element data), you don’t need to specify any arguments to `backward()`, however if it has more elements, you need to specify a `gradient` argument that is a tensor of matching shape."]},{"cell_type":"code","metadata":{"id":"WdBchdQ5u_us","colab_type":"code","colab":{}},"source":["import torch \n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tiP_QHxuXnav","colab_type":"text"},"source":["### What is `.data`?\n","\n","Read this [answer](https://stackoverflow.com/a/51744091/6644968) from stack overflow to get more insigths regarding `.data` attribute and also Pytorch's `Variable` wrapper which is deprecated now.\n","\n","Furthermore,\n","`.data` attribute is important for updating a tensor (whose `required_grads` attribute is set to `True`) during backpropagation. \n","\n","For ex : \n","\n","We cannot update `f` directly as follows:\n","\n","```\n","learning_rate = 0.01\n","for f in net.parameters():\n","    f.sub_(f.grad.data * learning_rate)\n","```\n","\n","It will throw the following error:\n","\n","```\n","RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.\n","```\n","\n","So instead we should be updating its `data` attribute as follows:\n","\n","```\n","learning_rate = 0.01\n","for f in net.parameters():\n","    f.data.sub_(f.grad.data * learning_rate)\n","```"]},{"cell_type":"code","metadata":{"id":"0qZg79gmXGg0","colab_type":"code","outputId":"4474d4e0-6219-4b2a-c5d4-356fea44c083","executionInfo":{"status":"ok","timestamp":1590385016254,"user_tz":-330,"elapsed":4532,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["test = torch.randn(5, 3)\n","print(test.data)\n","print(test.requires_grad)\n","test.requires_grad_() # This sets the requires_grad attribute to True for the tensor test in place.\n","print(test.requires_grad)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["tensor([[-0.3807, -0.2505, -0.3975],\n","        [ 0.2366,  0.2425, -0.7856],\n","        [ 0.5951, -0.0046,  0.3324],\n","        [-0.9676, -0.4793, -0.4148],\n","        [-0.0694,  2.3514, -1.2304]])\n","False\n","True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"d8bHMPgkSrOk","colab_type":"text"},"source":["### Difference between `torch.Tensor` and `torch.cuda.Tensor`\n","\n","Before we continue with the `autograd` package let us look at some of the differences between the above two.\n","This [answer](https://stackoverflow.com/a/53630326/6644968) clears quite a few doubts regarding the differences."]},{"cell_type":"code","metadata":{"id":"wA3SQhsHTcFK","colab_type":"code","outputId":"83db80d5-c65e-42ee-8f5a-0f8d713535a2","executionInfo":{"status":"ok","timestamp":1590385025522,"user_tz":-330,"elapsed":13763,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["# device will be 'cuda' if a GPU is available\n","# Try enabling GPU before running this cell\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# creating a CPU tensor\n","cpu_tensor = torch.rand(10)\n","# moving same tensor to GPU\n","gpu_tensor = cpu_tensor.to(device)\n","\n","print(cpu_tensor)\n","print(gpu_tensor)\n","print(\"*\" * 65)\n","\n","print(cpu_tensor.dtype, type(cpu_tensor), cpu_tensor.type(), cpu_tensor.device)\n","print(gpu_tensor.dtype, type(gpu_tensor), gpu_tensor.type(), gpu_tensor.device)\n","\n","# print(cpu_tensor * gpu_tensor)\n","# The above line throws the following error\n","# RuntimeError: Expected object of type torch.FloatTensor but found type torch.cuda.FloatTensor for argument #2 'other'"],"execution_count":3,"outputs":[{"output_type":"stream","text":["tensor([0.1740, 0.6659, 0.0572, 0.4530, 0.3958, 0.5776, 0.3269, 0.5295, 0.0776,\n","        0.0674])\n","tensor([0.1740, 0.6659, 0.0572, 0.4530, 0.3958, 0.5776, 0.3269, 0.5295, 0.0776,\n","        0.0674], device='cuda:0')\n","*****************************************************************\n","torch.float32 <class 'torch.Tensor'> torch.FloatTensor cpu\n","torch.float32 <class 'torch.Tensor'> torch.cuda.FloatTensor cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aY6JEAlHVv9x","colab_type":"text"},"source":["There are two ways to push the cpu_tensor (which currently resides in CPU) to GPU.\n","\n","1st Method: We can just change the `Tensor` type as follows:\n","\n","It will automatically change the `device` attribute too."]},{"cell_type":"code","metadata":{"id":"QzRQCdmEVDqF","colab_type":"code","outputId":"125292c6-f555-4d41-f5a7-7f07225cc724","executionInfo":{"status":"ok","timestamp":1590385025524,"user_tz":-330,"elapsed":13742,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["dtype = torch.cuda.FloatTensor\n","cpu_tensor = cpu_tensor.type(dtype)\n","\n","print(cpu_tensor.dtype, type(cpu_tensor), cpu_tensor.type(), cpu_tensor.device)\n","print(gpu_tensor.dtype, type(gpu_tensor), gpu_tensor.type(), gpu_tensor.device)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["torch.float32 <class 'torch.Tensor'> torch.cuda.FloatTensor cuda:0\n","torch.float32 <class 'torch.Tensor'> torch.cuda.FloatTensor cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5VG-sI7PWKCC","colab_type":"text"},"source":["2nd Method: We can directly change the `device` attribute as `\"cuda\"`.\n","This will automatically change the `Tensor` type."]},{"cell_type":"code","metadata":{"id":"8TlYX0Z1WVj_","colab_type":"code","outputId":"07958edf-c495-4e05-f98a-af393f628cc9","executionInfo":{"status":"ok","timestamp":1590385025526,"user_tz":-330,"elapsed":13721,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["dtype = torch.FloatTensor\n","cpu_tensor = cpu_tensor.type(dtype) # Pushing back to cpu from gpu\n","\n","cpu_tensor = cpu_tensor.to(torch.device(\"cuda:0\"))\n","print(cpu_tensor.dtype, type(cpu_tensor), cpu_tensor.type(), cpu_tensor.device)\n","print(gpu_tensor.dtype, type(gpu_tensor), gpu_tensor.type(), gpu_tensor.device)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["torch.float32 <class 'torch.Tensor'> torch.cuda.FloatTensor cuda:0\n","torch.float32 <class 'torch.Tensor'> torch.cuda.FloatTensor cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YAdFoGooW51L","colab_type":"text"},"source":["You can read more about the `Tensor` types and `Tensor` attributes here.\n","\n","[`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)\n","\n","[Tensor Attributes](https://pytorch.org/docs/stable/tensor_attributes.html)\n","\n","[Get datatype of Tensor](https://stackoverflow.com/questions/53374499/get-the-data-type-of-a-pytorch-tensor)"]},{"cell_type":"markdown","metadata":{"id":"DAtATh_hWvUl","colab_type":"text"},"source":["Bottomline is you can follow two neat methods to switch from CPU to GPU\n","\n","**Method 1:**\n","You can set the Tensor's type."]},{"cell_type":"code","metadata":{"id":"qWTyL0ueYEUL","colab_type":"code","outputId":"4e04f6c3-c2fe-4210-afae-748eae50e30f","executionInfo":{"status":"ok","timestamp":1590385025530,"user_tz":-330,"elapsed":13700,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n","print(torch.zeros(2, 2).type(dtype))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["tensor([[0., 0.],\n","        [0., 0.]], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mOAlX_a1Y06T","colab_type":"text"},"source":["**Method 2:** You can directly set the `device` attribute and use the `to()` method to switch between CPU and GPU\n","\n"]},{"cell_type":"code","metadata":{"id":"C3Jn3p0sY761","colab_type":"code","outputId":"774fc0bd-1cc6-419a-b0d1-9b8c9136470f","executionInfo":{"status":"ok","timestamp":1590385025534,"user_tz":-330,"elapsed":13683,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(torch.zeros(2, 2).to(device))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["tensor([[0., 0.],\n","        [0., 0.]], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cIXYJ_ViZMvU","colab_type":"text"},"source":["### Autograd\n","\n","Without deviating too much, lets get back to the PyTorch's `autograd` package which is the main objective of this notebook."]},{"cell_type":"markdown","metadata":{"id":"HH5Q6knxvOww","colab_type":"text"},"source":["Create a tensor and set `requires_grad=True` to track computation with it"]},{"cell_type":"code","metadata":{"id":"r9afu-DDvLKg","colab_type":"code","outputId":"ceda924b-6ccc-49bb-e564-2b32a03f3516","executionInfo":{"status":"ok","timestamp":1590385025537,"user_tz":-330,"elapsed":13663,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["x = torch.ones(2, 2, requires_grad = True)\n","print(x)\n","print(x.requires_grad)\n","print(x.data)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["tensor([[1., 1.],\n","        [1., 1.]], requires_grad=True)\n","True\n","tensor([[1., 1.],\n","        [1., 1.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1THgq38UvaQo","colab_type":"text"},"source":["Do a tensor operation:"]},{"cell_type":"code","metadata":{"id":"Z2mNrdW1vZDa","colab_type":"code","outputId":"d89a7162-143e-443c-8457-c6417e00525f","executionInfo":{"status":"ok","timestamp":1590385025539,"user_tz":-330,"elapsed":13645,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["y = x + 2\n","print(y)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["tensor([[3., 3.],\n","        [3., 3.]], grad_fn=<AddBackward0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BIfFA3Q_viO5","colab_type":"text"},"source":["`y` was created as a result of an operation, so it has a `grad_fn`."]},{"cell_type":"code","metadata":{"id":"yDx_P4pTvhVh","colab_type":"code","outputId":"f9d91bcb-007c-49fa-fad8-b2e768ae11a4","executionInfo":{"status":"ok","timestamp":1590385025540,"user_tz":-330,"elapsed":13625,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(y.grad_fn)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["<AddBackward0 object at 0x7f13c0f47a58>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2RhY3UA6vowo","colab_type":"text"},"source":["Do more operations on `y`"]},{"cell_type":"code","metadata":{"id":"loEU4VnZvp_Z","colab_type":"code","outputId":"84e852e5-e960-49ef-9243-56bafcd39d60","executionInfo":{"status":"ok","timestamp":1590385025542,"user_tz":-330,"elapsed":13611,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["z = y * y * 3\n","out = z.mean()\n","\n","print(z, out)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["tensor([[27., 27.],\n","        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"abu9JZ8DwFCD","colab_type":"text"},"source":["`.requires_grad_( ... )` changes an existing Tensor’s `requires_grad` flag in-place. The input flag defaults to `False` if not given."]},{"cell_type":"code","metadata":{"id":"835pC1zlwI-4","colab_type":"code","outputId":"ade39d61-2a71-4732-80da-2e59befd61f1","executionInfo":{"status":"ok","timestamp":1590385025544,"user_tz":-330,"elapsed":13593,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["a = torch.randn(2, 2)\n","a = ((a * 3) / (a - 1))\n","print(a.requires_grad)\n","a.requires_grad_(True)\n","print(a.requires_grad)\n","b = (a * a).sum()\n","print(b.grad_fn)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["False\n","True\n","<SumBackward0 object at 0x7f13bf3182b0>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x_ACHCETwY5B","colab_type":"text"},"source":["## Gradients\n","\n","Let’s backprop now. Because `out` contains a single scalar, `out.backward()` is equivalent to `out.backward(torch.tensor(1.))`."]},{"cell_type":"code","metadata":{"id":"kU-SN1GvwoHi","colab_type":"code","colab":{}},"source":["out.backward()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X5RODCu7wsmA","colab_type":"text"},"source":["Print gradients `d(out)/dx`"]},{"cell_type":"code","metadata":{"id":"fK7zXouNwvSw","colab_type":"code","outputId":"81fed361-9363-4dc9-80cf-6c81be3c7a96","executionInfo":{"status":"ok","timestamp":1590385025548,"user_tz":-330,"elapsed":13573,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["print(x.grad)\n","print(x.grad.data)\n","x.grad.data.zero_()\n","print(x.grad)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["tensor([[4.5000, 4.5000],\n","        [4.5000, 4.5000]])\n","tensor([[4.5000, 4.5000],\n","        [4.5000, 4.5000]])\n","tensor([[0., 0.],\n","        [0., 0.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0XAN10E6lQhW","colab_type":"text"},"source":["The result obtained can be verified as follows :\n","\n","![forward_prop](https://drive.google.com/uc?id=1DMHZ_fvfuH8k65ED-t6WSZ44aYc_yArA)\n","\n","![backprop](https://drive.google.com/uc?id=1GMwUkT7vdXRpVYmWX7nVqm3aLLOi17To)"]},{"cell_type":"markdown","metadata":{"id":"a46FbkP8ygg3","colab_type":"text"},"source":["![alt text](https://drive.google.com/uc?id=1-EDfqeFC4Rg9cQYSyWunlhrEKMn10RoJ)\n","\n","Now let’s take a look at an example of vector-Jacobian product:"]},{"cell_type":"code","metadata":{"id":"ZStH3fTmzT5L","colab_type":"code","outputId":"15ad29b9-8c66-46f8-e1ac-3ea960fbf1b5","executionInfo":{"status":"ok","timestamp":1590385025550,"user_tz":-330,"elapsed":13560,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["x = torch.randn(3, requires_grad = True)\n","\n","y = x * 2\n","while y.data.norm() < 1000:\n","    y = y * 2\n","\n","print(y)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["tensor([ -40.3429, -435.6156, 1240.3335], grad_fn=<MulBackward0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qwq7tIVzzg9F","colab_type":"text"},"source":["Now in this case `y` is no longer a scalar. `torch.autograd` could not compute the full Jacobian directly, but if we just want the vector-Jacobian product, simply pass the vector to `backward` as argument:"]},{"cell_type":"code","metadata":{"id":"OebO_48Uzun4","colab_type":"code","outputId":"3df1a01a-39a8-4593-a878-3c815466db35","executionInfo":{"status":"ok","timestamp":1590385025552,"user_tz":-330,"elapsed":13549,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n","y.backward(v)\n","\n","print(x.grad)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Lsu9QmuD0O6H","colab_type":"text"},"source":["You can also stop autograd from tracking history on Tensors with `.requires_grad=True` either by wrapping the code block in `with torch.no_grad():`"]},{"cell_type":"code","metadata":{"id":"redAZFh-0mEG","colab_type":"code","outputId":"753fdbf9-9e62-40c5-a49c-4ba5adde4322","executionInfo":{"status":"ok","timestamp":1590385025555,"user_tz":-330,"elapsed":13535,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["print(x.requires_grad)\n","print((x ** 2).requires_grad)\n","\n","with torch.no_grad():\n","    print((x ** 2).requires_grad)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["True\n","True\n","False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tFqDQYmi0rK4","colab_type":"text"},"source":["Or by using `.detach()` to get a new Tensor with the same content but that does not require gradients:"]},{"cell_type":"code","metadata":{"id":"38bQRklS0tQm","colab_type":"code","outputId":"84865b83-d4d8-4cb0-d322-126c183023e0","executionInfo":{"status":"ok","timestamp":1590385025557,"user_tz":-330,"elapsed":13524,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["print(x.requires_grad)\n","y = x.detach()\n","print(y.requires_grad)\n","print(x.requires_grad)\n","print(x.eq(y).all())"],"execution_count":18,"outputs":[{"output_type":"stream","text":["True\n","False\n","True\n","tensor(True)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"J14a0Mxd03EP","colab_type":"text"},"source":["**Read Later:**\n","\n","Documentation of `autograd.Function` can be found at https://pytorch.org/docs/stable/autograd.html#function"]},{"cell_type":"markdown","metadata":{"id":"x2zRzKNwHcOz","colab_type":"text"},"source":["# References\n","\n","1. [Deep Learning with PyTorch: A 60 minute blitz, Soumith Chintala](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n","\n","2. [Automatic Differentiation Package - TORCH.AUTOGRAD](https://pytorch.org/docs/stable/autograd.html)\n","\n","3. [TORCH](https://pytorch.org/docs/stable/torch.html)\n","\n","4. [Stefan Otte: Deep Neural Networks with PyTorch | PyData Berlin 2018](https://www.youtube.com/watch?v=_H3aw6wkCv0&t=821s)\n","\n","5. [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)\n","\n","6. [Tensor Attributes](https://pytorch.org/docs/stable/tensor_attributes.html)\n","\n","7. [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)"]}]}