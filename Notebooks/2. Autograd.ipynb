{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. Autograd.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOtVb1yUilJtAijmt4tfmau"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"T2TKdEOBG0BV","colab_type":"text"},"source":["# AUTOGRAD: Automatic Differentiation"]},{"cell_type":"markdown","metadata":{"id":"GNpmpYnZHIic","colab_type":"text"},"source":["The PyTorch's `autograd` package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different."]},{"cell_type":"markdown","metadata":{"id":"VDJRsZs6JSeJ","colab_type":"text"},"source":["## Tensors\n","\n","`torch.Tensor` is the central class of the package. If you set its attribute `.requires_grad` as `True`, it starts to track all operations on it. When you finish your computation you can call `.backward()` and have all the gradients computed automatically. The gradient for this tensor will be accumulated into `.grad` attribute.\n","\n","To stop a tensor from tracking history, you can call `.detach()` to detach it from the computation history, and to prevent future computation from being tracked.\n","\n","To prevent tracking history (and using memory), you can also wrap the code block in `with torch.no_grad():`. This can be particularly helpful when evaluating a model because the model may have trainable parameters with `requires_grad=True`, but for which we don’t need the gradients.\n","\n","There’s one more class which is very important for autograd implementation - a `Function`.\n","\n","`Tensor` and `Function` are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each tensor has a `.grad_fn` attribute that references a `Function` that has created the `Tensor` (except for Tensors created by the user - their `grad_fn is None`).\n","\n","If you want to compute the derivatives, you can call `.backward()` on a `Tensor`. If `Tensor` is a scalar (i.e. it holds one element data), you don’t need to specify any arguments to `backward()`, however if it has more elements, you need to specify a `gradient` argument that is a tensor of matching shape."]},{"cell_type":"code","metadata":{"id":"WdBchdQ5u_us","colab_type":"code","colab":{}},"source":["import torch \n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tiP_QHxuXnav","colab_type":"text"},"source":["### What is `.data`?\n","\n","Read this [answer](https://stackoverflow.com/a/51744091/6644968) from stack overflow to get more insigths regarding `.data` attribute and also Pytorch's `Variable` wrapper which is deprecated now.\n","\n","Furthermore,\n","`.data` attribute is important for updating a tensor (whose `required_grads` attribute is set to `True`) during backpropagation. \n","\n","For ex : \n","\n","We cannot update `f` directly as follows:\n","\n","```\n","learning_rate = 0.01\n","for f in net.parameters():\n","    f.sub_(f.grad.data * learning_rate)\n","```\n","\n","It will throw the following error:\n","\n","```\n","RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.\n","```\n","\n","So instead we should be updating its `data` attribute as follows:\n","\n","```\n","learning_rate = 0.01\n","for f in net.parameters():\n","    f.data.sub_(f.grad.data * learning_rate)\n","```"]},{"cell_type":"code","metadata":{"id":"0qZg79gmXGg0","colab_type":"code","outputId":"99c2dd2a-dd2f-4d73-e100-9a4bc0e7fffc","executionInfo":{"status":"ok","timestamp":1590639745159,"user_tz":-330,"elapsed":4208,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["test = torch.randn(5, 3)\n","print(test.data)\n","print(test.requires_grad)\n","test.requires_grad_() # This sets the requires_grad attribute to True for the tensor test in place.\n","print(test.requires_grad)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["tensor([[ 0.4510, -0.7793,  0.5411],\n","        [-1.1596,  0.2477,  0.2743],\n","        [-0.2677, -0.8608,  2.0140],\n","        [ 1.9691,  0.7678,  1.5181],\n","        [-0.6210,  0.4080, -0.9801]])\n","False\n","True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"d8bHMPgkSrOk","colab_type":"text"},"source":["### Difference between `torch.Tensor` and `torch.cuda.Tensor`\n","\n","Before we continue with the `autograd` package let us look at some of the differences between the above two.\n","This [answer](https://stackoverflow.com/a/53630326/6644968) clears quite a few doubts regarding the differences."]},{"cell_type":"code","metadata":{"id":"wA3SQhsHTcFK","colab_type":"code","outputId":"a3cc6bdd-61e7-4f22-83af-59e2da88f86b","executionInfo":{"status":"ok","timestamp":1590639754878,"user_tz":-330,"elapsed":13901,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["# device will be 'cuda' if a GPU is available\n","# Try enabling GPU before running this cell\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# creating a CPU tensor\n","cpu_tensor = torch.rand(10)\n","# moving same tensor to GPU\n","gpu_tensor = cpu_tensor.to(device)\n","\n","print(cpu_tensor)\n","print(gpu_tensor)\n","print(\"*\" * 65)\n","\n","print(cpu_tensor.dtype, type(cpu_tensor), cpu_tensor.type(), cpu_tensor.device)\n","print(gpu_tensor.dtype, type(gpu_tensor), gpu_tensor.type(), gpu_tensor.device)\n","\n","# print(cpu_tensor * gpu_tensor)\n","# The above line throws the following error\n","# RuntimeError: Expected object of type torch.FloatTensor but found type torch.cuda.FloatTensor for argument #2 'other'"],"execution_count":3,"outputs":[{"output_type":"stream","text":["tensor([0.7859, 0.8757, 0.2199, 0.0118, 0.8641, 0.2560, 0.8038, 0.8948, 0.2806,\n","        0.0846])\n","tensor([0.7859, 0.8757, 0.2199, 0.0118, 0.8641, 0.2560, 0.8038, 0.8948, 0.2806,\n","        0.0846], device='cuda:0')\n","*****************************************************************\n","torch.float32 <class 'torch.Tensor'> torch.FloatTensor cpu\n","torch.float32 <class 'torch.Tensor'> torch.cuda.FloatTensor cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aY6JEAlHVv9x","colab_type":"text"},"source":["There are two ways to push the cpu_tensor (which currently resides in CPU) to GPU.\n","\n","1st Method: We can just change the `Tensor` type as follows:\n","\n","It will automatically change the `device` attribute too."]},{"cell_type":"code","metadata":{"id":"QzRQCdmEVDqF","colab_type":"code","outputId":"916c3957-9b55-4fc3-c592-d0b110c049b1","executionInfo":{"status":"ok","timestamp":1590639754885,"user_tz":-330,"elapsed":13889,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["dtype = torch.cuda.FloatTensor\n","cpu_tensor = cpu_tensor.type(dtype)\n","\n","print(cpu_tensor.dtype, type(cpu_tensor), cpu_tensor.type(), cpu_tensor.device)\n","print(gpu_tensor.dtype, type(gpu_tensor), gpu_tensor.type(), gpu_tensor.device)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["torch.float32 <class 'torch.Tensor'> torch.cuda.FloatTensor cuda:0\n","torch.float32 <class 'torch.Tensor'> torch.cuda.FloatTensor cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5VG-sI7PWKCC","colab_type":"text"},"source":["2nd Method: We can directly change the `device` attribute as `\"cuda\"`.\n","This will automatically change the `Tensor` type."]},{"cell_type":"code","metadata":{"id":"8TlYX0Z1WVj_","colab_type":"code","outputId":"da3d62b5-3530-4c4d-cde1-aaaffb54bd6f","executionInfo":{"status":"ok","timestamp":1590639754888,"user_tz":-330,"elapsed":13874,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["dtype = torch.FloatTensor\n","cpu_tensor = cpu_tensor.type(dtype) # Pushing back to cpu from gpu\n","\n","cpu_tensor = cpu_tensor.to(torch.device(\"cuda:0\"))\n","print(cpu_tensor.dtype, type(cpu_tensor), cpu_tensor.type(), cpu_tensor.device)\n","print(gpu_tensor.dtype, type(gpu_tensor), gpu_tensor.type(), gpu_tensor.device)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["torch.float32 <class 'torch.Tensor'> torch.cuda.FloatTensor cuda:0\n","torch.float32 <class 'torch.Tensor'> torch.cuda.FloatTensor cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YAdFoGooW51L","colab_type":"text"},"source":["You can read more about the `Tensor` types and `Tensor` attributes here.\n","\n","[`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)\n","\n","[Tensor Attributes](https://pytorch.org/docs/stable/tensor_attributes.html)\n","\n","[Get datatype of Tensor](https://stackoverflow.com/questions/53374499/get-the-data-type-of-a-pytorch-tensor)"]},{"cell_type":"markdown","metadata":{"id":"DAtATh_hWvUl","colab_type":"text"},"source":["Bottomline is you can follow two neat methods to switch from CPU to GPU\n","\n","**Method 1:**\n","You can set the Tensor's type."]},{"cell_type":"code","metadata":{"id":"qWTyL0ueYEUL","colab_type":"code","outputId":"87d0be88-eb43-4b8b-a9fd-e2e5a110b185","executionInfo":{"status":"ok","timestamp":1590639754890,"user_tz":-330,"elapsed":13858,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n","print(torch.zeros(2, 2).type(dtype))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["tensor([[0., 0.],\n","        [0., 0.]], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mOAlX_a1Y06T","colab_type":"text"},"source":["**Method 2:** You can directly set the `device` attribute and use the `to()` method to switch between CPU and GPU\n","\n"]},{"cell_type":"code","metadata":{"id":"C3Jn3p0sY761","colab_type":"code","outputId":"20331cba-5888-45a0-ebe3-03b078cb248e","executionInfo":{"status":"ok","timestamp":1590639754892,"user_tz":-330,"elapsed":13840,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(torch.zeros(2, 2).to(device))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["tensor([[0., 0.],\n","        [0., 0.]], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cIXYJ_ViZMvU","colab_type":"text"},"source":["### Autograd\n","\n","Without deviating too much, lets get back to the PyTorch's `autograd` package which is the main objective of this notebook."]},{"cell_type":"markdown","metadata":{"id":"HH5Q6knxvOww","colab_type":"text"},"source":["Create a tensor and set `requires_grad=True` to track computation with it"]},{"cell_type":"code","metadata":{"id":"r9afu-DDvLKg","colab_type":"code","outputId":"d634aef3-2497-4a5d-e995-fb50e34fa183","executionInfo":{"status":"ok","timestamp":1590639754894,"user_tz":-330,"elapsed":13820,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["x = torch.ones(2, 2, requires_grad = True)\n","print(x)\n","print(x.requires_grad)\n","print(x.data)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["tensor([[1., 1.],\n","        [1., 1.]], requires_grad=True)\n","True\n","tensor([[1., 1.],\n","        [1., 1.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1THgq38UvaQo","colab_type":"text"},"source":["Do a tensor operation:"]},{"cell_type":"code","metadata":{"id":"Z2mNrdW1vZDa","colab_type":"code","outputId":"268ec448-30a8-43b2-bb78-b29bf124a638","executionInfo":{"status":"ok","timestamp":1590639754896,"user_tz":-330,"elapsed":13800,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["y = x + 2\n","print(y)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["tensor([[3., 3.],\n","        [3., 3.]], grad_fn=<AddBackward0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BIfFA3Q_viO5","colab_type":"text"},"source":["`y` was created as a result of an operation, so it has a `grad_fn`."]},{"cell_type":"code","metadata":{"id":"yDx_P4pTvhVh","colab_type":"code","outputId":"f173e5d5-71f6-4b53-ed20-df2daf5ed987","executionInfo":{"status":"ok","timestamp":1590639754900,"user_tz":-330,"elapsed":13786,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(y.grad_fn)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["<AddBackward0 object at 0x7fe8ce486b70>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2RhY3UA6vowo","colab_type":"text"},"source":["Do more operations on `y`"]},{"cell_type":"code","metadata":{"id":"loEU4VnZvp_Z","colab_type":"code","outputId":"8f4f5ead-6079-4e72-e514-a378a2a04543","executionInfo":{"status":"ok","timestamp":1590639754903,"user_tz":-330,"elapsed":13776,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["z = y * y * 3\n","out = z.mean()\n","\n","print(z, out)\n","\n","print(y.requires_grad)\n","print(z.requires_grad)\n","print(out.requires_grad)\n","# Notice that although we did not explicitly set the requires_grad attribute to\n","# \"True\" for the Tensors y, z and out, it is automatically set to True."],"execution_count":11,"outputs":[{"output_type":"stream","text":["tensor([[27., 27.],\n","        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n","True\n","True\n","True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"abu9JZ8DwFCD","colab_type":"text"},"source":["`.requires_grad_( ... )` changes an existing Tensor’s `requires_grad` flag in-place. The input flag defaults to `False` if not given."]},{"cell_type":"code","metadata":{"id":"835pC1zlwI-4","colab_type":"code","outputId":"40785523-1746-4752-d4f4-fcd22ee9a21a","executionInfo":{"status":"ok","timestamp":1590639754905,"user_tz":-330,"elapsed":13765,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["a = torch.randn(2, 2)\n","a = ((a * 3) / (a - 1))\n","print(a.requires_grad)\n","a.requires_grad_(True)\n","print(a.requires_grad)\n","b = (a * a).sum()\n","print(b.grad_fn)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["False\n","True\n","<SumBackward0 object at 0x7fe8c685b278>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x_ACHCETwY5B","colab_type":"text"},"source":["## Gradients\n","\n","Let’s backprop now. Because `out` contains a single scalar, `out.backward()` is equivalent to `out.backward(torch.tensor(1.))`."]},{"cell_type":"code","metadata":{"id":"kU-SN1GvwoHi","colab_type":"code","colab":{}},"source":["out.backward()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X5RODCu7wsmA","colab_type":"text"},"source":["Print gradients `d(out)/dx`"]},{"cell_type":"code","metadata":{"id":"fK7zXouNwvSw","colab_type":"code","outputId":"5ce57c32-b50f-4b32-a07b-b456471c6bb7","executionInfo":{"status":"ok","timestamp":1590639754909,"user_tz":-330,"elapsed":13753,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["print(x.grad)\n","print(x.grad.data)\n","x.grad.data.zero_()\n","print(x.grad)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["tensor([[4.5000, 4.5000],\n","        [4.5000, 4.5000]])\n","tensor([[4.5000, 4.5000],\n","        [4.5000, 4.5000]])\n","tensor([[0., 0.],\n","        [0., 0.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0XAN10E6lQhW","colab_type":"text"},"source":["The result obtained can be verified as follows :\n","\n","![forward_prop](https://drive.google.com/uc?id=1DMHZ_fvfuH8k65ED-t6WSZ44aYc_yArA)\n","\n","![backprop](https://drive.google.com/uc?id=1GMwUkT7vdXRpVYmWX7nVqm3aLLOi17To)"]},{"cell_type":"markdown","metadata":{"id":"a46FbkP8ygg3","colab_type":"text"},"source":["![alt text](https://drive.google.com/uc?id=1-EDfqeFC4Rg9cQYSyWunlhrEKMn10RoJ)\n","\n","Now let’s take a look at an example of vector-Jacobian product:"]},{"cell_type":"code","metadata":{"id":"ZStH3fTmzT5L","colab_type":"code","outputId":"94f29796-4971-4eb0-e385-6e505fac58dd","executionInfo":{"status":"ok","timestamp":1590639754910,"user_tz":-330,"elapsed":13745,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["x = torch.randn(3, requires_grad = True)\n","\n","y = x * 2\n","while y.data.norm() < 1000:\n","    y = y * 2\n","\n","print(y)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["tensor([-1048.9928,    -8.1988,    17.0514], grad_fn=<MulBackward0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qwq7tIVzzg9F","colab_type":"text"},"source":["Now in this case `y` is no longer a scalar. `torch.autograd` could not compute the full Jacobian directly, but if we just want the vector-Jacobian product, simply pass the vector to `backward` as argument:"]},{"cell_type":"code","metadata":{"id":"OebO_48Uzun4","colab_type":"code","outputId":"c38700c8-cfc4-4c55-dfdd-34271603a42c","executionInfo":{"status":"ok","timestamp":1590639754913,"user_tz":-330,"elapsed":13741,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n","y.backward(v)\n","\n","print(x.grad)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Lsu9QmuD0O6H","colab_type":"text"},"source":["You can also stop autograd from tracking history on Tensors with `.requires_grad=True` either by wrapping the code block in `with torch.no_grad():`"]},{"cell_type":"code","metadata":{"id":"redAZFh-0mEG","colab_type":"code","outputId":"43124fb2-ac11-4972-fa72-961c8df6cd3c","executionInfo":{"status":"ok","timestamp":1590639754916,"user_tz":-330,"elapsed":13735,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["print(x.requires_grad)\n","print((x ** 2).requires_grad)\n","\n","with torch.no_grad():\n","    print((x ** 2).requires_grad)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["True\n","True\n","False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tFqDQYmi0rK4","colab_type":"text"},"source":["Or by using `.detach()` to get a new Tensor with the same content but that does not require gradients:"]},{"cell_type":"code","metadata":{"id":"38bQRklS0tQm","colab_type":"code","outputId":"7595dc7b-99bb-48c0-a3ce-ba22db12b60c","executionInfo":{"status":"ok","timestamp":1590639754918,"user_tz":-330,"elapsed":13729,"user":{"displayName":"Darshan Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9rJ96_FUlkKwATDmDI4uA1p05C1MirD-yLWNyRg=s64","userId":"08607213967965377720"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["print(x.requires_grad)\n","y = x.detach()\n","print(y.requires_grad)\n","print(x.requires_grad)\n","print(x.eq(y).all())"],"execution_count":18,"outputs":[{"output_type":"stream","text":["True\n","False\n","True\n","tensor(True)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"J14a0Mxd03EP","colab_type":"text"},"source":["**Read Later:**\n","\n","Documentation of `autograd.Function` can be found at https://pytorch.org/docs/stable/autograd.html#function"]},{"cell_type":"markdown","metadata":{"id":"x2zRzKNwHcOz","colab_type":"text"},"source":["# References\n","\n","1. [Deep Learning with PyTorch: A 60 minute blitz, Soumith Chintala](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n","\n","2. [Automatic Differentiation Package - TORCH.AUTOGRAD](https://pytorch.org/docs/stable/autograd.html)\n","\n","3. [TORCH](https://pytorch.org/docs/stable/torch.html)\n","\n","4. [Stefan Otte: Deep Neural Networks with PyTorch | PyData Berlin 2018](https://www.youtube.com/watch?v=_H3aw6wkCv0&t=821s)\n","\n","5. [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)\n","\n","6. [Tensor Attributes](https://pytorch.org/docs/stable/tensor_attributes.html)\n","\n","7. [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)"]}]}